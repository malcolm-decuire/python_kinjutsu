{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from pandas import read_csv\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# seperate array into input & output components\n",
    "X = array [:,0:8]\n",
    "Y = array [:,8]\n",
    "binarizer = Binarizer(threshold=0.0) .fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "#summarize transformed data\n",
    "set_printoptions(precision=4)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH 8: Feature Selection\n",
    "## 3 Reasons For Feature Selection\n",
    "1. Overfitting - less redudant data means less likely to decided based on noise\n",
    "2. Improves Accuracy - less noisey data means data is closer to accurate depiction\n",
    "3. Reduces Resource Allocation- less data means algo can go faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "#8.2 Univariate Selection Feature Selection Tests \n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# seperate array into input & output components\n",
    "X = array [:,0:8]\n",
    "Y = array [:,8]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH 8.3 Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to find the top features, we will be using RFE to find the combo of attrs. that contribute the most to predicting the target attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example below uses RFE with the logistic regression algorithm to select the top 3 features. This will return an array of of true/false for each column & will rank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcd/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_features_to_select=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#8.3 Linear Algebra & Logistic Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# seperate array into input & output components\n",
    "X = array [:,0:8]\n",
    "Y = array [:,8]\n",
    "#feature extraction\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH 8.4 Principal Component Analysis\n",
    "### PCA uses linear algebra jutsu to do data reduction via transfrom + compression. PCA allows you to pick the number of dimensions/principal compnts. to transform. Today we are using 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.88854663 0.06159078 0.02579012]\n",
      "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
      "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
      " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
      "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
      " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
      "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]\n"
     ]
    }
   ],
   "source": [
    "#8.4 PCA, use alegbra to do data reduction  and find top 3 tings\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# seperate array into input & output components\n",
    "X = array [:,0:8]\n",
    "Y = array [:,8]\n",
    "#feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "#summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 Feature Importance\n",
    "### We will use Extra Tree decision-tree to classify diabetes in our dataset. Remember a feature's score, the more important. Below it will be clear that plas, age, and mass are key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11142076 0.23352594 0.0994475  0.07891256 0.07571193 0.13813235\n",
      " 0.12051233 0.14233662]\n"
     ]
    }
   ],
   "source": [
    "#8.5 Feat importance using Extra Treee Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# seperate array into input & output components\n",
    "X = array [:,0:8]\n",
    "Y = array [:,8]\n",
    "#feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X,Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.6 Summary on Feature building\n",
    "1. Univariate Selection - Data scientists can use SelectKbest & f_classif to select which attributes are most important. Today we found the top 4: preq,plas,mass,age\n",
    "\n",
    "2. Recursive Feature Elimination- Data Scientists can use RFE to remove uneccesary atts and model those that remain. Combo the RFE + logisitic regression to get top 3 features\n",
    "\n",
    "3. Principial Component Analysis- Data Sci folks must take datasets and do some data reduction. PCA allows folks to pick and choose which # of dimensions to transform\n",
    "\n",
    "4. Feature Importance - DS folks can use ExtraTreeClassifier to find which attrs are most important. The bigger the # the more likely its important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
